---
title: 机器学习开放课程：六、特征工程和特征选取
date: 2018-11-12 08:12:47
updated: 2018-11-12
tags:
 - 编程
 - Python
 - 机器学习
 - sklearn.feature_extraction.text.CountVectorizer
 - pytesseract
 - OCR tool for python
 - geopy
 - tsfresh
 - scipy.stats.shapiro
 - 零假设
 - p值
 - alpha级别
categories:
 - 编程
 - Python
 - 机器学习
typora-root-url: 机器学习开放课程：六、特征工程和特征选取
mathjax: true
description: <!—more—->
---

### 特征提取

#### 文本

在处理文本之前，必须tokenzie文本，也就是将文本切分为单元（token）。在最简单的情形下，token不过是单词。**因为直接按照单次来切分可能会损失一些信息，所以现在切分时更多的会考虑语言的特性（也就是语义和语境）。**

接下来需要正则化数据。对文本而言，这涉及词干提取（stemming）和词形还原（lemmatization）；这是用来处理同一单词的不同形式的类似过程。

[信息检索导论.pdf](信息检索导论.pdf)

{% blockquote 信息检索导论, Christopher D. Manning,Prabhakar Raghavan %}
词干还原（stemming）和词形归并（lemmatization）这两个术语所代表的意义是不同的。

前者通常指的是一个很粗略的去除单词两端词缀的启发式过程，并且希望大部分时间它都能达到这个正确目的，这个过程也常常包括去除派生词缀。

而词形归并通常指利用词汇表和词形分析来去除屈折词缀，从而返回词的原形或词典中的词的过程，返回的结果称为词元（lemma）。

假如给定词条 saw，词干还原过程可能仅返回 s，而词形归并过程将返回 see 或者 saw，当然具体返回哪个词**取决于在当前上下文**中 saw 到底是动词还是名词。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E8%AF%8D%E5%B9%B2%E6%8F%90%E5%8F%96 词干提取 %}

一个面向英语的词干提取器，例如，要识别字符串“cats”、“catlike”和“catty”是基于词根“cat”；“stemmer”、“stemming”和“stemmed”是基于词根“stem”。一根词干提取算法可以简化词 “fishing”、“fished”、“fish”和“fisher” 为同一个词根“fish”。

{% endblockquote %}

{% blockquote https://en.wikipedia.org/wiki/Lemmatisation Lemmatisation %}

在计算语言学中，lemmatisation是基于其预期含义确定单词的[引理](https://en.wikipedia.org/wiki/Lemma_(morphology))的算法过程。与[词干化](https://en.wikipedia.org/wiki/Stemming)不同，词汇化取决于正确识别句子中的预期[词性](https://en.wikipedia.org/wiki/Part_of_speech)和词语的含义，以及围绕该句子的较大语境，例如邻近句子甚至整个文档。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B 词袋模型 %}

此模型下，一段文本（比如一个句子或是一个文档）可以用一个装着这些词的袋子来表示，这种表示方式不考虑文法以及词的顺序。

1. John likes to watch movies. Mary likes movies too.
2. John also likes to watch football games.

基于以上两个文件，可以建构出下列清单:

```
[
    "John0",
    "likes",
    "to",
    "watch",
    "movies",
    "also",
    "football",
    "games",
    "Mary",
    "too"
]
```

此处有10个不同的词，使用清单的索引表示长度为10的向量，分别对应上面的两段文本。

1. `[1, 2, 1, 1, 2, 0, 0, 0, 1, 1]`
2. `[1, 1, 1, 1, 0, 1, 1, 1, 0, 0]`

每个向量的索引内容对应到清单中词出现的次数。

此向量表示法不会保存原始句子中词的顺序。该表示法有许多成功的应用，像是邮件过滤。

{% endblockquote %}

当使用类似词袋的算法时，我们丢失了文本中的单词顺序信息，这意味着向量化之后，“i have no cows”（我没有牛）和“no, i have cows”（没，我有牛）会变得一样，尽管事实上它们的意思截然相反。为了避免这个问题，我们可以转用[N元语法](https://zh.wikipedia.org/wiki/N%E5%85%83%E8%AF%AD%E6%B3%95)。

```python CountVectorizer示例 https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html sklearn.feature_extraction.text.CountVectorizer
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> corpus = [
...     'This is the first document.',
...     'This document is the second document.',
...     'And this is the third one.',
...     'Is this the first document?',
... ]
>>> vectorizer = CountVectorizer()
>>> X = vectorizer.fit_transform(corpus)
>>> print(vectorizer.get_feature_names())
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
>>> print(vectorizer.vocabulary_)
{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}
>>> print(X.toarray())  
[[0 1 1 1 0 0 1 0 1]
 [0 2 0 1 0 1 1 0 1]
 [1 0 0 1 1 0 1 1 1]
 [0 1 1 1 0 0 1 0 1]]
```

#### 图像

{% blockquote https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C 卷积神经网络 %}

卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和[池化](https://zh.wikipedia.org/w/index.php?title=%E6%B1%A0%E5%8C%96&action=edit&redlink=1)层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和[语音识别](https://zh.wikipedia.org/wiki/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB)方面能够给出更好的结果。

**卷积层**

卷积层（Convolutional layer），卷积神经网上中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过[反向传播算法](https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95)最优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网路能从低级特征中迭代提取更复杂的特征。

**池化层(Pooling Layer)**

![img](314px-Max_pooling.png)

每隔2个元素进行的2x2最大池化

池化（Pooling）是卷积神经网络中另一个重要的概念，它实际上是一种形式的降采样。有多种不同形式的非线性池化函数，而其中**“最大池化（Max pooling）”**是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。

直觉上，这种机制能够有效地原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了[过拟合](https://zh.wikipedia.org/wiki/%E8%BF%87%E6%8B%9F%E5%90%88)。通常来说，CNN的卷积层之间都会周期性地插入池化层。

池化层通常会分别作用于每个输入的特征并减小其大小。目前最常用形式的池化层是每隔2个元素从图像划分出$2\times2$的区块，然后对每个区块中的4个数取最大值。这将会减少75%的数据量。

除了最大池化之外，池化层也可以使用其他池化函数，例如“平均池化”甚至“[L2-范数](https://zh.wikipedia.org/wiki/Lp%E7%A9%BA%E9%97%B4#.E9.95.BF.E5.BA.A6.E3.80.81.E8.B7.9D.E7.A6.BB.E4.B8.8E.E8.8C.83.E6.95.B0)池化”等。过去，平均池化的使用曾经较为广泛，但是最近由于最大池化在实践中的表现更好，平均池化已经不太常用。

*由于池化层过快地减少了数据的大小，目前文献中的趋势是使用较小的池化滤镜，甚至不再使用池化层。*

{% endblockquote %}

[pytesseract](https://github.com/madmaze/pytesseract)是python的光学字符识别（OCR）工具。也就是说，它将识别并“读取”嵌入图像中的文本。

#### 地理空间数据

可以通过访问外部API（谷歌地图或OpenStreetMap）进行 *地理编码（由地址重建坐标点）*和*逆地理编码（由坐标点重建地址）*的互逆操作。很幸运，[geopy](https://github.com/geopy/geopy)之类的通用库封装了这些外部服务。

>  处理地理编码时，别忘了地址可能包含错误，因此需要清洗数据。坐标的错误更少，但由于GPS噪声或特定地点（比如隧道、商业区）的低精确度，可能导致位置不正确。如果数据源是移动设备，地理位置可能并不是由GPS决定的，而是由该区域的WiFi网络决定的，这会导致空间中的空洞和远距离传送。
>
> 地点常常位于许多基础设施之间。因此，你可以充分发挥想象力，基于生活经验和领域知识发明特征：地点到地铁口的接近程度，建筑物中的商户数，到最近的商店的距离，周边的ATM数目，等等。就任何任务而言，你很容易想到几十个特征并从不同的外部资源中提取它们。就城市以外的问题而言，你可以考虑来自更专门的数据源的特征，比如海拔。
>
> 如果两个以上的地名相互连接，可能有必要基于地点之间的路由提取特征。比如，距离（直线距离和基于路由图计算得出的道路距离），转弯数（包括左转和右转的比例），红路灯数，交叉路口数，桥梁数。在我自己遇到的一个任务中，我生成了一个称为“道路复杂度”的特征，该特征计算基于图得出的距离除以最大公因数。

#### 日期和时间

> 一般而言，处理时序数据时，最好有一份包含公众节假日、异常天气情况及其他重要事件的日历。

#### 时序，web，等等

对于时序的处理可以参考：[tsfresh](https://github.com/blue-yonder/tsfresh) *基于可扩展假设检验的时间序列特征提取*

对于 `user-agents` 的处理可以参考：[python-user-agents](https://github.com/selwin/python-user-agents)

下一个有用的特征是IP地址，基于该数据可以提取国家，乃至城市、网络运营商、连接类型（是否为移动网络）。**但是代理和数据库过期导致该特征可能包含噪声。**

> IP地址数据和`Accept-Language`是一对良好的组合：如果用户的IP地址显示在智利，而浏览器的本地化设为`ru_RU`（俄罗斯），该用户的所在地并不清晰，需要查看对应的特征栏`is_traveler_or_proxy_user`（是旅行者还是代理用户）。

### 特征转换

#### 正则化和改变分布

##### 对数据做正态性检验

**[零假设](https://zh.wikipedia.org/wiki/%E9%9B%B6%E5%81%87%E8%AE%BE)（$H_{0}$）**是普遍接受的事实。比如说抛硬币正反面出现的可能性均为50%。

> 零假设可以被认为是*可归因的*假设。这意味着您可以使其无效或拒绝它。

**替代假设（对立假设、[备择假设](https://zh.wikipedia.org/w/index.php?title=%E5%A4%87%E6%8B%A9%E5%81%87%E8%AE%BE&action=edit&redlink=1)）**。是零假设的替代。比如说你提出抛硬币正反面出现的可能性分别是30%和70%。那么你提出的这个观点就是替代假设。

**拒绝零假设**：证明零假设是错误的，这时使用**替代假设**所替代。

{% blockquote https://www.statisticshowto.datasciencecentral.com/support-or-reject-null-hypothesis/ What does it mean to reject the null hypothesis? %}

例如，假设您认为某种药物可能导致一系列近期心脏病发作。该制药公司认为这种药物是安全的。零假设始终是公认的假设; 在这个例子中，药物在市场上，人们正在使用它，并且通常认为它是安全的。因此，零假设是药物是安全的。另一种假设 - 你想要取代零假设的假设，就是药物*不是*安全。在这种情况下拒绝零假设意味着您必须证明药物不安全。

{% endblockquote %}

###### p值定义

在假设检验中使用 p值 来帮助支持或拒绝零假设。p值对于零假设来说是一个证据。**p值越小，拒绝零假设的证据就越强。**

p值表示为小数，但如果将它们转换为百分比，可能更容易理解它们是什么。

> 例如，p值0.0254是2.54％。这意味着您的结果有可能是2.54％随机的（即偶然发生）。
>
> 另一方面，大的p值为.9（90％）意味着您的结果有90％的概率是完全随机的，而不是由于实验中的任何内容。
>
> 因此，p值越小，结果越重要（“显着”）。

运行假设检验时，将测试中的p值与运行测试时选择的alpha值进行比较。alpha值也可以写为百分比。

###### p值与alpha级别

alpha级别由研究人员控制，并与置信水平相关。通过从100％减去置信度来获得alpha级别。

> 例如，如果您希望对您的研究有98％的信心，那么α水平将为2％（100％ - 98％）。
>
> 当您运行假设检验时，测试将为您提供p的值。将该值与您选择的alpha级别进行比较。
>
> 例如，假设您选择了5％（0.05）的alpha级别。如果测试结果给你：
>
> * 小p（≤0.05）拒绝零假设。这是零假设无效的有力证据。
> * 大p（> 0.05）表示备用假设很弱，因此您不会拒绝零假设。

###### 如果我没有Alpha级别怎么办？

在理想的世界中，你将拥有一个alpha级别。但是如果你没有的话，仍然可以使用以下粗略的指导方针来决定是否支持或拒绝原假设：

- 如果`p>.10`→“不重要”
- 如果`p≤.10`→“略显重要”
- 如果`p≤0.55`→“显着”
- 如果`p≤0.01`→“非常重要”。

{% blockquote https://www.statisticshowto.datasciencecentral.com/what-is-an-alpha-level/#alphawhy Why is an Alpha Level of .05 commonly used? %}

###### 为什么常用的alpha级别为.05？

看到alpha级别是产生Type I错误的概率，我们认为这个区域尽可能小是合理的。例如，如果我们将alpha级别设置为10％，那么我们很可能会错误地拒绝原假设，而1％的alpha级别会使该区域变小。那么为什么不使用一个小区域而不是标准的5％呢？

alpha级别越小，拒绝原假设的区域越小。因此，如果你有一个很小的区域，那么你有可能不会拒绝空位，实际上你应该这样做。这是Type II错误。

换句话说，你尝试避免类型I错误的次数越多，第二类错误就越容易发生。科学家们发现5％的α水平是这两个问题之间的良好平衡。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E7%AC%AC%E4%B8%80%E5%9E%8B%E5%8F%8A%E7%AC%AC%E4%BA%8C%E5%9E%8B%E9%8C%AF%E8%AA%A4 第一型及第二型錯誤 %}

* 若零假设事实上成立，但统计检验的结果不支持零假设（拒绝零假设），这种错误称为**第一型错误**。

* 若零假设事实上不成立，但统计检验的结果支持零假设（接受零假设），这种错误称为**第二型错误**。

以利用验孕棒验孕为例，此时未怀孕为零假设。

若用验孕棒为一位未怀孕的女士验孕，结果是已怀孕，这是**第一型错误**。

若用验孕棒为一位孕妇验孕，结果是未怀孕，这是**第二型错误**。

{% endblockquote %}

##### 正态性检验示例

**[`scipy.stats.shapiro`](http://scipy.github.io/devdocs/generated/scipy.stats.shapiro.html)**

> 对于N> 5000，W检验统计量是准确的，但p值可能不是。

```python shapiro https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html scipy.stats.shapiro
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import shapiro

np.random.seed(17)
mu, sigma = 0, 0.1
s = np.random.normal(mu, sigma, 1000)
count, bins, ignored = plt.hist(s, 30, density=True)
plt.plot(bins, 1 / (sigma * np.sqrt(2 * np.pi)) *
         np.exp(- (bins - mu) ** 2 / (2 * sigma ** 2)),
         linewidth=2, color='r')
plt.show()

print(shapiro(s))
```

![1542007030968](1542007030968.png)

```
(0.9982456564903259, 0.401098370552063)
```

上述代码得到的 p值 已经远远大于 0.1 了，表示这个结果完全不用采纳。维持原有 零假设 也就是这是一个正态分布的假设。













