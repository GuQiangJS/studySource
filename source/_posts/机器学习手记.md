---
title: 机器学习手记
date: 2018-11-09 08:27:54
updated: 2018-11-12
tags:
 - 编程
 - Python
 - 机器学习
 - sklearn.preprocessing.MinMaxScaler
 - sklearn.preprocessing.StandardScaler
categories:
 - 编程
 - Python
 - 机器学习
typora-root-url: 机器学习手记
mathjax: true
description: <!—more—->
---

### 机器学习的分类

{% blockquote http://sklearn.apachecn.org/cn/0.19.0/tutorial/basic/tutorial.html 机器学习：问题设置 %}

- [监督学习](https://en.wikipedia.org/wiki/Supervised_learning) , 其中数据带有一个附加属性，即我们想要预测的结果值（ [点击此处](http://sklearn.apachecn.org/cn/0.19.0/supervised_learning.html#supervised-learning) 转到 scikit-learn 监督学习页面）。这个问题可以是:

  - [分类](https://en.wikipedia.org/wiki/Classification_in_machine_learning) : 样本属于两个或更多个类，我们想从已经标记的数据中学习如何预测未标记数据的类别。 分类问题的一个例子是手写数字识别，其目的是将每个输入向量分配给有限数目的离散类别之一。 我们通常把分类视作监督学习的一个离散形式（区别于连续形式），从有限的类别中，给每个样本贴上正确的标签。
  - [回归](https://en.wikipedia.org/wiki/Regression_analysis) : 如果期望的输出由一个或多个连续变量组成，则该任务称为 *回归* 。 回归问题的一个例子是利用历史数据判断未来天气温度数据。

- [无监督学习](https://en.wikipedia.org/wiki/Unsupervised_learning), 其中训练数据由没有任何相应目标值的一组输入向量x组成。这种问题的目标可能是在数据中发现彼此类似的示例所聚成的组，这种问题称为 [聚类](https://en.wikipedia.org/wiki/Cluster_analysis) , 或者，确定输入空间内的数据分布，称为 [密度估计](https://en.wikipedia.org/wiki/Density_estimation) ，又或从高维数据投影数据空间缩小到二维或三维以进行 *可视化* （[点击此处](http://sklearn.apachecn.org/cn/0.19.0/unsupervised_learning.html#unsupervised-learning) 转到 scikit-learn 无监督学习页面）。

{% endblockquote %}

[选择正确的评估器(estimator)](http://sklearn.apachecn.org/cn/0.19.0/tutorial/machine_learning_map/index.html#)

### 垃圾进——垃圾出

{% blockquote https://zh.wikipedia.org/wiki/%E5%9E%83%E5%9C%BE%E8%BF%9B%EF%BC%8C%E5%9E%83%E5%9C%BE%E5%87%BA 垃圾进，垃圾出%}

著名的“垃圾进——垃圾出”概念100%适用于机器学习的任何任务。在高质量数据上训练的简单模型，表现优于在未清理的数据上训练的复杂多模型集成。

{% endblockquote %}

### 特征缩放

网上中文资料对于 *归一化*，*标准化* 的解释比较混乱。[Feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) 看英文版本较好。下面是我自己整理的解释。

首先是个人对于这两个定义的理解：

* 归一化首先确定最大最小的范围，使得数据在范围内缩放。
* 标准化首先确定中间值，然后向两侧发散。

#### 正则化/归一化（Normalization）

##### Rescaling (min-max normalization) 重新缩放（min-max 正则化）

把数据变为（0，1）之间的小数。主要是为了方便数据处理，因为**将数据映射到0～1范围之内**，可以使处理过程更加便捷、快速。

${x}'=\frac{x-min\left ( x \right )}{max\left ( x \right )-min\left ( x \right )}$

```python MinMaxScaler https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html sklearn.preprocessing.MinMaxScaler
>>> from sklearn.preprocessing import MinMaxScaler
>>>
>>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
>>> scaler = MinMaxScaler()
>>> print(scaler.fit(data))
MinMaxScaler(copy=True, feature_range=(0, 1))
>>> print(scaler.data_max_)
[ 1. 18.]
>>> print(scaler.transform(data))
[[0.   0.  ]
 [0.25 0.25]
 [0.5  0.5 ]
 [1.   1.  ]]
>>> print(scaler.transform([[2, 2]]))
[[1.5 0. ]]
```

##### Mean normalization 均值归一化

${x}'=\frac{x-average\left ( x \right )}{max\left ( x \right )-min\left ( x \right )}$

#### 标准化（Standardization）

特征标准化使得数据中每个特征的值具有**零均值**（当减去分子中的平均值时）和**单位方差**。

> 使得不同度量之间的特征具有可比性。同时不改变原始数据的分布。

${x}'=\frac{x - \bar{x}}{\sigma}$

其中 $\bar {x}={\text{average}}(x)$， $\sigma$ 是标准差。

```python StandardScaler https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler sklearn.preprocessing.StandardScaler
>>> from sklearn.preprocessing import StandardScaler
>>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]
>>> scaler = StandardScaler()
>>> print(scaler.fit(data))
StandardScaler(copy=True, with_mean=True, with_std=True)
>>> print(scaler.mean_)
[0.5 0.5]
>>> print(scaler.transform(data))
[[-1. -1.]
 [-1. -1.]
 [ 1.  1.]
 [ 1.  1.]]
>>> print(scaler.transform([[2, 2]]))
[[3. 3.]]
```

