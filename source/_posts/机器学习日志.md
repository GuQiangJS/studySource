---
title: mlcourse.ai 机器学习日志
date: 2018-11-15 16:25:23
update: 2018-11-26
tags:
 - 编程
 - Python
 - 机器学习
 - mlcourse.ai
 - 忽略警告
 - svg格式
 - matplotlib
 - 图形
 - seaborn
 - 中位数
 - PCA
 - 梯度下降
 - 驻点
 - 鞍点
 - 损失函数
categories:
 - 机器学习
mathjax: true
description: <!—more—->
typora-root-url: 机器学习日志
---

参加了 [mlcourse.ai](https://mlcourse.ai/) 的**开放式机器学习课程** [Yorko](https://github.com/Yorko)/[**mlcourse.ai**](https://github.com/Yorko/mlcourse.ai)。以下为每周的学习内容。

因为之前不太清楚相关规则和内容，前几周的内容都是后面补充的。

### 第一周 使用Pandas进行探索性数据分析

**教程：**

* [简易教程](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_pandas_data_analysis.ipynb)
* [简易教程 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_pandas_data_analysis_zh-cn.ipynb)

教程中演示了 `pandas` 的一些常用的方法。包含 读取数据、数据类型转换、排序、`describe`、索引、定位、`apply`、分组、`crosstab`、`pivot_table`等。以及少量的 `matplotlib` 的使用。

**教程内提供的相关网站链接：**

* [pandas官方文档](http://pandas.pydata.org/pandas-docs/stable/index.html)
* [pandas官方提供的10分钟入门](http://pandas.pydata.org/pandas-docs/stable/10min.html) 常用的 `pandas` 语法都包含了，也是个速查的入口。
* [pandas主要功能速查pdf](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf) **好东西**
* [scipy-lectures.org](http://www.scipy-lectures.org/index.html) 关于`pandas`、`numpy`、`matplotlib`和`scikit-learn`的教程

**任务：**

* [任务-使用Pandas进行探索性数据分析](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment01_pandas_uci_adult.ipynb)
* [任务-使用Pandas进行探索性数据分析 - 机翻中文，包含我自己的答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment01_pandas_uci_adult_zh-cn.ipynb)

主要是之前教程部分的练习。

**实践：**

* [实践 分析“泰坦尼克号”乘客](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_practice_pandas_titanic.ipynb)
* [实践 分析“泰坦尼克号”乘客 - 机翻中文，包含我自己的答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_practice_pandas_titanic_zh-cn.ipynb)
* [实践 分析“泰坦尼克号”乘客 包含官方答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_practice_pandas_titanic_solution.ipynb)

### 第二周 Python中的可视化数据分析

**教程：**

* [第1部分：可视化：从简单分布到降维](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_visual_data_analysis.ipynb)
* [第1部分：可视化：从简单分布到降维 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_visual_data_analysis_zh-cn.ipynb)

通过使用图形来分析电信用户流失的可能性来讲述了单变量可视化，多变量可视化、降维（PCA、t-SNE）

[淺談降維方法中的 PCA 與 t-SNE](https://medium.com/d-d-mag/%E6%B7%BA%E8%AB%87%E5%85%A9%E7%A8%AE%E9%99%8D%E7%B6%AD%E6%96%B9%E6%B3%95-pca-%E8%88%87-t-sne-d4254916925b)

**教程内提供的相关网站链接：**

- 这是我们使用的库的官方文档：[`matplotlib`](https://matplotlib.org/contents.html)，[`seaborn`](https://seaborn.pydata.org/introduction.html)和[`pandas`](https://pandas.pydata.org/pandas-docs/stable/)。
- 使用`seaborn`创建的示例图表的[gallery](http://seaborn.pydata.org/examples/index.html)是一个非常好的资源。
- 另请参阅`scikit-learn`中关于Manifold Learning的[documentation](http://scikit-learn.org/stable/modules/manifold.html)。
- 高效的t-SNE实施[Multicore-TSNE](https://github.com/DmitryUlyanov/Multicore-TSNE)。
- `如何有效使用t-SNE`，[Distill.pub](https://distill.pub/2016/misread-tsne/)。

---

* [第2部分：Seaborn，Matplotlib和Plotly库概述](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_additional_seaborn_matplotlib_plotly.ipynb)
* [第2部分：Seaborn，Matplotlib和Plotly库概述 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_additional_seaborn_matplotlib_plotly_zh-cn.ipynb)

通过示例简单描述了[pandas.DataFrame.plot](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html)，

### Seaborn

##### [`seaborn.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html)多变量之间的关系图

```python
# `pairplot()` may become very slow with the SVG format
%config InlineBackend.figure_format = 'png' 
sns.pairplot(df[['Global_Sales', 'Critic_Score', 'Critic_Count', 
                 'User_Score', 'User_Count']]);
```

![1542334908488](1542334908488.png)

##### [`seaborn.distplot`](https://seaborn.pydata.org/generated/seaborn.distplot.html?highlight=distplot#seaborn.distplot)

> 此功能将matplotlib `hist`功能（具有良好的默认箱尺寸的自动计算）与seaborn [`kdeplot()`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html#seaborn.kdeplot) 和[`rugplot()`](https://seaborn.pydata.org/generated/seaborn.rugplot.html#seaborn.rugplot)功能相结合。它还可以拟合`scipy.stats` 分布并在数据上绘制估计的PDF。

```python
%config InlineBackend.figure_format = 'svg'
sns.distplot(df['Critic_Score']);
```

![1542334908489](1542334908489.png)

##### [`seaborn.jointplot`](https://seaborn.pydata.org/generated/seaborn.jointplot.html?highlight=jointplot#seaborn.jointplot)

> 用双变量和单变量图绘制两个变量的图。

```python
sns.jointplot(x='Critic_Score', y='User_Score', 
              data=df, kind='scatter');
```

![1542334908490](1542334908490.png)

##### [`seaborn.boxplot`](https://seaborn.pydata.org/generated/seaborn.boxplot.html?highlight=boxplot#seaborn.boxplot) 箱型图

> 绘制方框图以显示与类别相关的分布。

```python
top_platforms = df['Platform'].value_counts().sort_values(ascending=False).head(5).index.values
sns.boxplot(y="Platform", x="Critic_Score", 
            data=df[df['Platform'].isin(top_platforms)], orient="h");
```

![1542334908491](1542334908491.png)

{% blockquote https://zh.wikipedia.org/wiki/%E7%AE%B1%E5%BD%A2%E5%9C%96 箱型图%}

```
                            +-----+-+       
  *           o     |-------|   + | |---|
                            +-----+-+    
                                         
+---+---+---+---+---+---+---+---+---+---+   分数
0   1   2   3   4   5   6   7   8   9  10
```

这组数据显示出：

- 最小值(*minimum*)=5
- 下四分位数(*Q1*)=7
- 中位数(*Med* --也就是Q2)=8.5
- 上四分位数(*Q3*)=9
- 最大值(*maximum* )=10
- 平均值=8
- 四分位间距(interquartile range)=$(Q3-Q1)$=2 (即ΔQ)

在区间 Q3+1.5ΔQ, Q1-1.5ΔQ 之外的值被视为应忽略(farout)。

- farout: 在图上不予显示，仅标注一个符号∇。

- 最大值区间： Q3+1.5ΔQ
- 最小值区间： Q1-1.5ΔQ

最大值与最小值产生于这个区间。区间外的值被视为outlier显示在图上.

- mild outlier （离群值) = 3.5
- extreme outlier (极端值) = 0.5

{% endblockquote %}

##### [`seaborn.heatmap`](https://seaborn.pydata.org/generated/seaborn.heatmap.html?highlight=heatmap#seaborn.heatmap)

> 将矩形数据绘制为颜色编码矩阵。

```python
platform_genre_sales = df.pivot_table(
                        index='Platform', 
                        columns='Genre', 
                        values='Global_Sales', 
                        aggfunc=sum).fillna(0).applymap(float)
sns.heatmap(platform_genre_sales, annot=True, fmt=".1f", linewidths=.5);
```

![1542334908492](1542334908492.png)

```python
# 禁止在python运行时显示警告信息
import warnings
warnings.filterwarnings('ignore')
```

```python
# 使用svg格式绘图，会更清晰
%config InlineBackend.figure_format = 'svg'
```

#### [Plotly](https://plot.ly/python/) [github](https://github.com/plotly/documentation/tree/source-design-merge/_posts/python/)

可交互式图形库。

![20181120095220](20181120095220.png)

**任务：**

* [任务 - 分析心血管疾病数据](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment02_analyzing_cardiovascular_desease_data.ipynb)
* [任务 - 分析心血管疾病数据 - 机翻中文，包含我自己的答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment02_analyzing_cardiovascular_desease_data_zh-cn.ipynb)
* [任务 - 分析心血管疾病数据 包含官方答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment02_analyzing_cardiovascular_desease_data_solution.ipynb)

{% blockquote https://zh.wikipedia.org/wiki/%E4%B8%AD%E4%BD%8D%E6%95%B8 中位数%}

**中位数**（又称**中值**，英语：Median），[统计学](https://zh.wikipedia.org/wiki/%E7%B5%B1%E8%A8%88%E5%AD%B8)中的专有名词，代表一个样本、种群或[概率分布](https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83)中的一个数值，其可将数值集合划分为相等的上下两部分。对于有限的数集，可以通过把所有观察值高低排序后找出正中间的一个作为中位数。如果观察值有偶数个，则中位数不唯一，通常取最中间的两个数值的[平均数](https://zh.wikipedia.org/wiki/%E5%B9%B3%E5%9D%87%E6%95%B0)作为中位数。

{% endblockquote %}

- [皮尔逊积矩相关系数](https://zh.wikipedia.org/wiki/%E7%9A%AE%E5%B0%94%E9%80%8A%E7%A7%AF%E7%9F%A9%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0)
- [pandas.DataFrame.corr](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html)

![1542761704983](1542761704983.png)

---

* [任务 - 美国航班的探索性数据分析（EDA）（使用Pandas，Matplotlib和Seaborn）](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_fall2018/assignment2_USA_flights.ipynb)
* 任务 - 美国航班的探索性数据分析（EDA）（使用Pandas，Matplotlib和Seaborn） - 机翻中文，包含我自己的答案

**实践：**

- [实践 分析“泰坦尼克号”乘客](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_practice_visual_titanic.ipynb)
- 实践 分析“泰坦尼克号”乘客 - 机翻中文，包含我自己的答案
- [实践 分析“泰坦尼克号”乘客 包含官方答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_practice_visual_titanic_solution.ipynb)



### 第七周 无监督学习：PCA和聚类

{% blockquote https://zh.wikipedia.org/wiki/%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92 非监督式学习 %}

**非监督式学习**是一种[机器学习](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)的方式，并不需要人力来输入标签。它是[监督式学习](https://zh.wikipedia.org/wiki/%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92)和[强化学习](https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)等策略之外的一种选择。

在监督式学习中，典型的任务是分类和[回归分析](https://zh.wikipedia.org/wiki/%E8%BF%B4%E6%AD%B8%E5%88%86%E6%9E%90)，且需要使用到人工预先准备好的范例(examples)。

一个常见的非监督式学习是[数据聚类](https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB)。在[人工神经网络](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)中，[生成对抗网络](https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C)（GAN）、[自组织映射](https://zh.wikipedia.org/wiki/%E8%87%AA%E7%BB%84%E7%BB%87%E6%98%A0%E5%B0%84)（SOM）和[适应性共振理论](https://zh.wikipedia.org/w/index.php?title=%E9%81%A9%E6%87%89%E6%80%A7%E5%85%B1%E6%8C%AF%E7%90%86%E8%AB%96&action=edit&redlink=1)（ART）则是最常用的非监督式学习。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E7%84%A1%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E7%B6%B2%E8%B7%AF 无监督式学习网络 %}

**无监督式学习网络**(Unsupervised Learning Network)是人工智能网络的一种算法(algorithm)，其目的是去对原始资料进行分类，以便了解资料内部结构。有别于[监督式学习网络](https://zh.wikipedia.org/wiki/%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E7%B6%B2%E8%B7%AF)，无监督式学习网络在学习时并不知道其分类结果是否正确，亦即没有受到监督式增强(告诉它何种学习是正确的)。其特点是仅对此种网络提供输入范例，而它会自动从这些范例中找出其潜在类别规则。当学习完毕并经测试后，也可以将之应用到新的案例上。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90 主成分分析 %}

在多元统计分析中，**主成分分析**（英语：**Principal components analysis**，**PCA**）是一种分析、简化数据集的技术。主成分分析经常用于减少数据集的[维数](https://zh.wikipedia.org/wiki/%E7%BB%B4%E6%95%B0)，同时保持数据集中的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。但是，这也不是一定的，要视具体应用而定。由于主成分分析依赖所给数据，所以数据的准确性对分析结果影响很大。

{% endblockquote %}

**PCA的[问答](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)示例**

* [无监督学习-PCA和群集](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/topic7_pca_clustering.ipynb)
* [无监督学习-PCA和群集 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/topic7_pca_clustering_zh-cn.ipynb)
* [无监督学习-第一部分 主成分分析](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/lesson7_part1_PCA.ipynb)
* [无监督学习-第一部分 主成分分析 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/lesson7_part1_PCA_zh-cn.ipynb)

- [无监督学习-第二部分 类聚 K-均值](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/lesson7_part2_kmeans.ipynb)
- [无监督学习-第二部分 类聚 K-均值 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/lesson7_part2_kmeans_zh-cn.ipynb)

- [无监督学习-作业（demo）](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment07_unsupervised_learning.ipynb)
- [无监督学习-作业（demo） - 机翻中文，包含我自己的答案]
- [无监督学习-作业-主成分分析和聚类](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_fall2018/assignment7_pca_clustering.ipynb)
- [无监督学习-作业-主成分分析和聚类 - 机翻中文，包含我自己的答案]

### 第八周 使用Vowpal Wabbit高速学习大规模数据集

**教程：**

- [Vowpal Wabbit：学习千兆字节的数据](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic08_sgd_hashing_vowpal_wabbit/topic8_sgd_hashing_vowpal_wabbit.ipynb)
- [Vowpal Wabbit：学习千兆字节的数据 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic08_sgd_hashing_vowpal_wabbit/topic8_sgd_hashing_vowpal_wabbit_zh-cn.ipynb)

{% blockquote https://zh.wikipedia.org/zh-hans/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95 梯度下降法 %}

**梯度下降法**（英语：Gradient descent）是一个一阶[最优化](https://zh.wikipedia.org/wiki/%E6%9C%80%E4%BC%98%E5%8C%96)[算法](https://zh.wikipedia.org/wiki/%E7%AE%97%E6%B3%95)，通常也称为**最速下降法**。 要使用梯度下降法找到一个函数的[局部极小值](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%80%BC)，必须向函数上当前点对应[梯度](https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6)（或者是近似梯度）的*反方向*的规定步长距离点进行[迭代](https://zh.wikipedia.org/wiki/%E8%BF%AD%E4%BB%A3)搜索。如果相反地向梯度*正方向*迭代进行搜索，则会接近函数的[局部极大值](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%80%BC)点；这个过程则被称为**梯度上升法**。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E9%9E%8D%E9%BB%9E 鞍点 %}

一个不是[局部极值点](https://zh.wikipedia.org/wiki/%E6%9E%81%E5%80%BC)的[驻点](https://zh.wikipedia.org/wiki/%E9%A7%90%E9%BB%9E)称为鞍点。

![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Saddle_point.png/220px-Saddle_point.png)

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E9%A9%BB%E7%82%B9 驻点 %}

![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Stationary_vs_inflection_pts.svg/350px-Stationary_vs_inflection_pts.svg.png)

$y = x + \sin(2x)$ 的图像
驻点（红色）与[拐点](https://zh.wikipedia.org/wiki/%E6%8B%90%E7%82%B9)（蓝色），这图像的驻点都是局部极大值或局部极小值。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E6%8B%90%E7%82%B9 拐点 %}

![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/X_cubed_%28narrow%29.svg/220px-X_cubed_%28narrow%29.svg.png)

$y=x3$的函数图形，原点是其拐点。

**拐点**（Inflection point）或称**反曲点**，是一条可微[曲线](https://zh.wikipedia.org/wiki/%E6%9B%B2%E7%BA%BF)改变[凹凸性](https://zh.wikipedia.org/wiki/%E5%87%B9%E5%87%B8%E6%80%A7)的点，或者等价地说，是使[切线](https://zh.wikipedia.org/wiki/%E5%88%87%E7%B7%9A)穿越曲线的点。

{% endblockquote %}

{% blockquote https://en.wikipedia.org/wiki/Stochastic_gradient_descent 从英文翻译而来-随机梯度下降 %}

**随机梯度下降**（通常缩短为**SGD**），也称为**增量**梯度下降，是用于[优化](https://en.wikipedia.org/wiki/Mathematical_optimization)可[微分](https://en.wikipedia.org/wiki/Differentiable_function)[目标函数](https://en.wikipedia.org/wiki/Objective_function)的[迭代方法](https://en.wikipedia.org/wiki/Iterative_method)，[梯度下降](https://en.wikipedia.org/wiki/Gradient_descent)优化的[随机近似](https://en.wikipedia.org/wiki/Stochastic_approximation)。最近的一篇文章[[1\]](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#cite_note-1)暗示[Herbert Robbins](https://en.wikipedia.org/wiki/Herbert_Robbins)和Sutton Monro在其1951年题为“随机近似方法”的文章中发展了SGD。有关更多信息，请参阅[随机近似](https://en.wikipedia.org/wiki/Stochastic_approximation)。它被称为**随机的** 因为样本是随机选择（或混洗）而不是作为单个组（如标准[梯度下降](https://en.wikipedia.org/wiki/Gradient_descent)）或按训练集中出现的顺序选择的。

{% endblockquote %}

步长（有时称为机器学习中的学习率）

{% blockquote https://wiki.mbalib.com/wiki/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0 损失函数 %}

**损失函数的公式**

$L=k(y-m)2$

* $L$——以美元表示的单位损失（惩罚成本）

* $y$——质量变量的值

* $m$——质量变量y的目标值

* $k$——常数，取决于质量变量在财务上的重要性。

---

**损失函数的相关案例**

　　某匹萨外卖店承诺在接到订单30分钟内[送货](https://wiki.mbalib.com/wiki/%E9%80%81%E8%B4%A7)。如果超过承诺时间10分钟，那么将赔偿金额3美元。送货成本大概是2美元，但是，随着送货时间的推迟，送货成本将以0.15美元/分钟的速度下降。实际送货时间应偏离目标送货时间多长，能使成本最低？

　　总成本（$N$）=送货成本（$J$）+赔偿成本（$L$）

　　设实际送货时间为y

　　送货成本 $J=2-0.15(y-30)$

　　赔偿成本$L=k(y-30)2$

　　$N=L+J=k(y-30)2+[2-0.15(y-30)]$

　　首先求出$k$，再求出$Min(N)$。如何求$k$?

　　$L=k(y-30)2$

　　$3=k(10-0)2$

　　![\frac{3}{10^2}=0.03](https://wiki.mbalib.com/w/images/math/8/8/1/8815b78c05e842fec0e1b7dd8e1c36bd.png)

　　$Min(N)=Min \{ 0.03(y-30）2+[2-0.15(y-30)]\}$

　　求导，得出$y-30=2.5$,即实际送货时间不偏离目标时间2.5分钟以上。

{% endblockquote %}