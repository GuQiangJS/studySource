---
title: mlcourse.ai 机器学习日志
date: 2018-11-15 16:25:23
update: 2018-11-29
tags:
 - 编程
 - Python
 - 机器学习
 - mlcourse.ai
 - 忽略警告
 - svg格式
 - matplotlib
 - 图形
 - seaborn
 - 中位数
 - PCA
 - 梯度下降
 - 驻点
 - 鞍点
 - 损失函数
 - 分类
 - 决策树
 - K邻居
 - 香农熵
 - 基尼不纯度（基尼不确定性）
 - sklearn.tree.DecisionTreeClassifier
 - sklearn.neighbors.KNeighborsClassifier
 - sklearn.ensemble.RandomForestClassifier
 - sklearn.model_selection.GridSearchCV
 - sklearn.model_selection.cross_val_score
 - sklearn.pipeline.Pipeline
categories:
 - 机器学习
mathjax: true
description: <!—more—->
typora-root-url: 机器学习日志
---

参加了 [mlcourse.ai](https://mlcourse.ai/) 的**开放式机器学习课程** [Yorko](https://github.com/Yorko)/[**mlcourse.ai**](https://github.com/Yorko/mlcourse.ai)。以下为每周的学习内容。

因为之前不太清楚相关规则和内容，前几周的内容都是后面补充的。

### 第一周 使用Pandas进行探索性数据分析

#### 教程

* [简易教程](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_pandas_data_analysis.ipynb)
* [简易教程 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_pandas_data_analysis_zh-cn.ipynb)

教程中演示了 `pandas` 的一些常用的方法。包含 读取数据、数据类型转换、排序、`describe`、索引、定位、`apply`、分组、`crosstab`、`pivot_table`等。以及少量的 `matplotlib` 的使用。

#### 教程内提供的相关网站链接

* [pandas官方文档](http://pandas.pydata.org/pandas-docs/stable/index.html)
* [pandas官方提供的10分钟入门](http://pandas.pydata.org/pandas-docs/stable/10min.html) 常用的 `pandas` 语法都包含了，也是个速查的入口。
* [pandas主要功能速查pdf](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf) **好东西**
* [scipy-lectures.org](http://www.scipy-lectures.org/index.html) 关于`pandas`、`numpy`、`matplotlib`和`scikit-learn`的教程

#### 任务

* [任务-使用Pandas进行探索性数据分析](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment01_pandas_uci_adult.ipynb)
* [任务-使用Pandas进行探索性数据分析 - 机翻中文，包含我自己的答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment01_pandas_uci_adult_zh-cn.ipynb)

主要是之前教程部分的练习。

#### 实践

* [实践 分析“泰坦尼克号”乘客](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_practice_pandas_titanic.ipynb)
* [实践 分析“泰坦尼克号”乘客 - 机翻中文，包含我自己的答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_practice_pandas_titanic_zh-cn.ipynb)
* [实践 分析“泰坦尼克号”乘客 包含官方答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_practice_pandas_titanic_solution.ipynb)

---

### 第二周 Python中的可视化数据分析

#### 教程

* [第1部分：可视化：从简单分布到降维](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_visual_data_analysis.ipynb)
* [第1部分：可视化：从简单分布到降维 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_visual_data_analysis_zh-cn.ipynb)

通过使用图形来分析电信用户流失的可能性来讲述了单变量可视化，多变量可视化、降维（PCA、t-SNE）

[淺談降維方法中的 PCA 與 t-SNE](https://medium.com/d-d-mag/%E6%B7%BA%E8%AB%87%E5%85%A9%E7%A8%AE%E9%99%8D%E7%B6%AD%E6%96%B9%E6%B3%95-pca-%E8%88%87-t-sne-d4254916925b)

#### 教程内提供的相关网站链接

- 这是我们使用的库的官方文档：[`matplotlib`](https://matplotlib.org/contents.html)，[`seaborn`](https://seaborn.pydata.org/introduction.html)和[`pandas`](https://pandas.pydata.org/pandas-docs/stable/)。
- 使用`seaborn`创建的示例图表的[gallery](http://seaborn.pydata.org/examples/index.html)是一个非常好的资源。
- 另请参阅`scikit-learn`中关于Manifold Learning的[documentation](http://scikit-learn.org/stable/modules/manifold.html)。
- 高效的t-SNE实施[Multicore-TSNE](https://github.com/DmitryUlyanov/Multicore-TSNE)。
- `如何有效使用t-SNE`，[Distill.pub](https://distill.pub/2016/misread-tsne/)。

---

* [第2部分：Seaborn，Matplotlib和Plotly库概述](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_additional_seaborn_matplotlib_plotly.ipynb)
* [第2部分：Seaborn，Matplotlib和Plotly库概述 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_additional_seaborn_matplotlib_plotly_zh-cn.ipynb)

通过示例简单描述了[pandas.DataFrame.plot](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html)，

#### [Seaborn](https://seaborn.pydata.org/api.html)

![20181129134038](20181129134038.png)

```python
# 禁止在python运行时显示警告信息
import warnings
warnings.filterwarnings('ignore')
```

```python
# 使用svg格式绘图，会更清晰
%config InlineBackend.figure_format = 'svg'
```

#### [Plotly](https://plot.ly/python/) [github](https://github.com/plotly/documentation/tree/source-design-merge/_posts/python/)

可交互式图形库。

![20181120095220](20181120095220.png)

#### 任务

* [任务 - 分析心血管疾病数据](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment02_analyzing_cardiovascular_desease_data.ipynb)
* [任务 - 分析心血管疾病数据 - 机翻中文，包含我自己的答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment02_analyzing_cardiovascular_desease_data_zh-cn.ipynb)
* [任务 - 分析心血管疾病数据 包含官方答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment02_analyzing_cardiovascular_desease_data_solution.ipynb)

{% blockquote https://zh.wikipedia.org/wiki/%E4%B8%AD%E4%BD%8D%E6%95%B8 中位数%}

**中位数**（又称**中值**，英语：Median），[统计学](https://zh.wikipedia.org/wiki/%E7%B5%B1%E8%A8%88%E5%AD%B8)中的专有名词，代表一个样本、种群或[概率分布](https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83)中的一个数值，其可将数值集合划分为相等的上下两部分。对于有限的数集，可以通过把所有观察值高低排序后找出正中间的一个作为中位数。如果观察值有偶数个，则中位数不唯一，通常取最中间的两个数值的[平均数](https://zh.wikipedia.org/wiki/%E5%B9%B3%E5%9D%87%E6%95%B0)作为中位数。

{% endblockquote %}

- [皮尔逊积矩相关系数](https://zh.wikipedia.org/wiki/%E7%9A%AE%E5%B0%94%E9%80%8A%E7%A7%AF%E7%9F%A9%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0)
- [pandas.DataFrame.corr](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) - 计算列的成对相关性

![1542761704983](1542761704983.png)

---

* [实践 分析“泰坦尼克号”乘客](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_practice_visual_titanic.ipynb)
* [实践 分析“泰坦尼克号”乘客 - 机翻中文，包含我自己的答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_practice_visual_titanic_zh-cn.ipynb)
* [实践 分析“泰坦尼克号”乘客 包含官方答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_practice_visual_titanic_solution.ipynb)

主要实践了部分 `seaborn` 的绘图方法。

#### 实践

- [任务 - 美国航班的探索性数据分析（EDA）（使用Pandas，Matplotlib和Seaborn）](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_fall2018/assignment2_USA_flights.ipynb)
- [任务 - 美国航班的探索性数据分析（EDA）（使用Pandas，Matplotlib和Seaborn） - 机翻中文，包含我自己的答案](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_fall2018/assignment2_USA_flights_zh-cn.ipynb)

---

### 第三周 分类，决策树和k个最近邻居

* [主题3.分类，决策树和k个最近邻居](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic03_decision_trees_kNN/topic3_decision_trees_kNN.ipynb)
* [主题3.分类，决策树和k个最近邻居 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic03_decision_trees_kNN/topic3_decision_trees_kNN_zh-cn.ipynb)

{% blockquote https://zh.wikipedia.org/wiki/C4.5%E7%AE%97%E6%B3%95 C4.5算法 %}

**C4.5算法**是由[Ross Quinlan](https://zh.wikipedia.org/w/index.php?title=Ross_Quinlan&action=edit&redlink=1)开发的用于产生[决策树](https://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91)的算法。该算法是对Ross Quinlan之前开发的[ID3算法](https://zh.wikipedia.org/wiki/ID3%E7%AE%97%E6%B3%95)的一个扩展。C4.5算法产生的决策树可以被用作分类目的，因此该算法也可以用于[统计分类](https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%88%86%E7%B1%BB)。

C4.5算法与ID3算法一样使用了[信息熵](https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E7%86%B5)的概念，并和ID3一样通过学习数据来建立决策树。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E9%A6%99%E5%86%9C%E7%86%B5 香农熵 %}

**熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。**

当取自有限的样本时，熵的公式可以表示为：

$\mathrm{H}\left ( X \right )=-\sum_{i}^{ }P\left ( x_{i} \right )\log_{b}P\left ( x_{i} \right )$

在这里 $b$ 是[对数](https://zh.wikipedia.org/wiki/%E5%B0%8D%E6%95%B8)所使用的[底](https://zh.wikipedia.org/wiki/%E5%BA%95%E6%95%B0_(%E5%AF%B9%E6%95%B0))，通常是2,自然常数[e](https://zh.wikipedia.org/wiki/E_(%E6%95%B0%E5%AD%A6%E5%B8%B8%E6%95%B0))，或是10。当*b* = 2，熵的单位是[bit](https://zh.wikipedia.org/wiki/%E4%BD%8D%E5%85%83)；当*b* = e，熵的单位是[nat](https://zh.wikipedia.org/wiki/%E5%A5%88%E7%89%B9_(%E5%8D%95%E4%BD%8D))；而当 $b$ = 10,熵的单位是Hart。

**在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。**

**范例：**

如英语有26个字母，假如每个字母在文章中出现次数平均的话，每个字母的讯息量为：

$I_{e}=-\log _{2}{1 \over 26}=4.7$

以日文五十音平假名作为相对范例，假设每个平假名日语文字在文章中出现的概率相等，每个平假名日语文字可携带的信息量为：

${\displaystyle I_{e}=-\log _{2}{1 \over 50}=5.64}$

而汉字常用的有2500个，假如每个汉字在文章中出现次数平均的话，每个汉字的信息量为：

$I_{e}=-\log _{2}{1 \over 2500}=11.3$

*如果两个系统具有同样大的消息量，如一篇用不同文字写的同一文章，由于汉字的信息量较大，中文文章应用的汉字就比英文文章使用的字母要少。所以汉字印刷的文章要比其他应用总体数量少的字母印刷的文章要短。即使一个汉字占用两个字母的空间，汉字印刷的文章也要比英文字母印刷的用纸少。*

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0#%E5%9F%BA%E5%B0%BC%E4%B8%8D%E7%BA%AF%E5%BA%A6%E6%8C%87%E6%A0%87 基尼不纯度指标 %}

在CART（Classification And Regression Tree，即分类回归树算法，简称CART算法，它是决策树的一种实现）算法中, 基尼不纯度表示一个随机选中的样本在子集中被分错的可能性。

基尼不纯度为这个样本被选中的概率乘以它被分错的概率。当一个节点中所有样本都是一个类时，基尼不纯度为零。

假设 $y$ 的可能取值为{1, 2, ..., m},令 $fi$ 是样本被赋予 $i$ 的概率，则基尼指数可以通过如下计算：

$I_{G} \left ( f \right )=1- \sum_{i=1}^{m} {f_{i}}^{2} $

{% endblockquote %}

代码方面介绍了

* [`sklearn.tree.DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
* [`sklearn.neighbors.KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)
* [`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
* sklearn.model_selection.GridSearchCV
* sklearn.model_selection.cross_val_score
* sklearn.pipeline.Pipeline

**使用K近邻方式时，大多数这些指标都需要缩放数据。**

#### 演示

* [演示 - 带有玩具任务和UCI Adult数据集的决策树](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment03_decision_trees.ipynb)
* [演示 - 带有玩具任务和UCI Adult数据集的决策树 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment03_decision_trees_zh-cn.ipynb)



---

### 第七周 无监督学习：PCA和聚类

{% blockquote https://zh.wikipedia.org/wiki/%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92 非监督式学习 %}

**非监督式学习**是一种[机器学习](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)的方式，并不需要人力来输入标签。它是[监督式学习](https://zh.wikipedia.org/wiki/%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92)和[强化学习](https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)等策略之外的一种选择。

在监督式学习中，典型的任务是分类和[回归分析](https://zh.wikipedia.org/wiki/%E8%BF%B4%E6%AD%B8%E5%88%86%E6%9E%90)，且需要使用到人工预先准备好的范例(examples)。

一个常见的非监督式学习是[数据聚类](https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB)。在[人工神经网络](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)中，[生成对抗网络](https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C)（GAN）、[自组织映射](https://zh.wikipedia.org/wiki/%E8%87%AA%E7%BB%84%E7%BB%87%E6%98%A0%E5%B0%84)（SOM）和[适应性共振理论](https://zh.wikipedia.org/w/index.php?title=%E9%81%A9%E6%87%89%E6%80%A7%E5%85%B1%E6%8C%AF%E7%90%86%E8%AB%96&action=edit&redlink=1)（ART）则是最常用的非监督式学习。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E7%84%A1%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E7%B6%B2%E8%B7%AF 无监督式学习网络 %}

**无监督式学习网络**(Unsupervised Learning Network)是人工智能网络的一种算法(algorithm)，其目的是去对原始资料进行分类，以便了解资料内部结构。有别于[监督式学习网络](https://zh.wikipedia.org/wiki/%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E7%B6%B2%E8%B7%AF)，无监督式学习网络在学习时并不知道其分类结果是否正确，亦即没有受到监督式增强(告诉它何种学习是正确的)。其特点是仅对此种网络提供输入范例，而它会自动从这些范例中找出其潜在类别规则。当学习完毕并经测试后，也可以将之应用到新的案例上。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90 主成分分析 %}

在多元统计分析中，**主成分分析**（英语：**Principal components analysis**，**PCA**）是一种分析、简化数据集的技术。主成分分析经常用于减少数据集的[维数](https://zh.wikipedia.org/wiki/%E7%BB%B4%E6%95%B0)，同时保持数据集中的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。但是，这也不是一定的，要视具体应用而定。由于主成分分析依赖所给数据，所以数据的准确性对分析结果影响很大。

{% endblockquote %}

**PCA的[问答](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)示例**

* [无监督学习-PCA和群集](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/topic7_pca_clustering.ipynb)
* [无监督学习-PCA和群集 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/topic7_pca_clustering_zh-cn.ipynb)
* [无监督学习-第一部分 主成分分析](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/lesson7_part1_PCA.ipynb)
* [无监督学习-第一部分 主成分分析 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/lesson7_part1_PCA_zh-cn.ipynb)

- [无监督学习-第二部分 类聚 K-均值](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/lesson7_part2_kmeans.ipynb)
- [无监督学习-第二部分 类聚 K-均值 - 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic07_unsupervised/lesson7_part2_kmeans_zh-cn.ipynb)

- [无监督学习-作业（demo）](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_demo/assignment07_unsupervised_learning.ipynb)
- [无监督学习-作业（demo） - 机翻中文，包含我自己的答案]
- [无监督学习-作业-主成分分析和聚类](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/assignments_fall2018/assignment7_pca_clustering.ipynb)
- [无监督学习-作业-主成分分析和聚类 - 机翻中文，包含我自己的答案]

### 第八周 使用Vowpal Wabbit高速学习大规模数据集

**教程：**

- [Vowpal Wabbit：学习千兆字节的数据](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic08_sgd_hashing_vowpal_wabbit/topic8_sgd_hashing_vowpal_wabbit.ipynb)
- [Vowpal Wabbit：学习千兆字节的数据 机翻中文](https://nbviewer.jupyter.org/github/GuQiangJS/mlcourse.ai/blob/master/jupyter_english/topic08_sgd_hashing_vowpal_wabbit/topic8_sgd_hashing_vowpal_wabbit_zh-cn.ipynb)

{% blockquote https://zh.wikipedia.org/zh-hans/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95 梯度下降法 %}

**梯度下降法**（英语：Gradient descent）是一个一阶[最优化](https://zh.wikipedia.org/wiki/%E6%9C%80%E4%BC%98%E5%8C%96)[算法](https://zh.wikipedia.org/wiki/%E7%AE%97%E6%B3%95)，通常也称为**最速下降法**。 要使用梯度下降法找到一个函数的[局部极小值](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%80%BC)，必须向函数上当前点对应[梯度](https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6)（或者是近似梯度）的*反方向*的规定步长距离点进行[迭代](https://zh.wikipedia.org/wiki/%E8%BF%AD%E4%BB%A3)搜索。如果相反地向梯度*正方向*迭代进行搜索，则会接近函数的[局部极大值](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%80%BC)点；这个过程则被称为**梯度上升法**。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E9%9E%8D%E9%BB%9E 鞍点 %}

一个不是[局部极值点](https://zh.wikipedia.org/wiki/%E6%9E%81%E5%80%BC)的[驻点](https://zh.wikipedia.org/wiki/%E9%A7%90%E9%BB%9E)称为鞍点。

![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Saddle_point.png/220px-Saddle_point.png)

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E9%A9%BB%E7%82%B9 驻点 %}

![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Stationary_vs_inflection_pts.svg/350px-Stationary_vs_inflection_pts.svg.png)

$y = x + \sin(2x)$ 的图像
驻点（红色）与[拐点](https://zh.wikipedia.org/wiki/%E6%8B%90%E7%82%B9)（蓝色），这图像的驻点都是局部极大值或局部极小值。

{% endblockquote %}

{% blockquote https://zh.wikipedia.org/wiki/%E6%8B%90%E7%82%B9 拐点 %}

![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/X_cubed_%28narrow%29.svg/220px-X_cubed_%28narrow%29.svg.png)

$y=x3$的函数图形，原点是其拐点。

**拐点**（Inflection point）或称**反曲点**，是一条可微[曲线](https://zh.wikipedia.org/wiki/%E6%9B%B2%E7%BA%BF)改变[凹凸性](https://zh.wikipedia.org/wiki/%E5%87%B9%E5%87%B8%E6%80%A7)的点，或者等价地说，是使[切线](https://zh.wikipedia.org/wiki/%E5%88%87%E7%B7%9A)穿越曲线的点。

{% endblockquote %}

{% blockquote https://en.wikipedia.org/wiki/Stochastic_gradient_descent 从英文翻译而来-随机梯度下降 %}

**随机梯度下降**（通常缩短为**SGD**），也称为**增量**梯度下降，是用于[优化](https://en.wikipedia.org/wiki/Mathematical_optimization)可[微分](https://en.wikipedia.org/wiki/Differentiable_function)[目标函数](https://en.wikipedia.org/wiki/Objective_function)的[迭代方法](https://en.wikipedia.org/wiki/Iterative_method)，[梯度下降](https://en.wikipedia.org/wiki/Gradient_descent)优化的[随机近似](https://en.wikipedia.org/wiki/Stochastic_approximation)。最近的一篇文章[[1\]](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#cite_note-1)暗示[Herbert Robbins](https://en.wikipedia.org/wiki/Herbert_Robbins)和Sutton Monro在其1951年题为“随机近似方法”的文章中发展了SGD。有关更多信息，请参阅[随机近似](https://en.wikipedia.org/wiki/Stochastic_approximation)。它被称为**随机的** 因为样本是随机选择（或混洗）而不是作为单个组（如标准[梯度下降](https://en.wikipedia.org/wiki/Gradient_descent)）或按训练集中出现的顺序选择的。

{% endblockquote %}

步长（有时称为机器学习中的学习率）

{% blockquote https://wiki.mbalib.com/wiki/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0 损失函数 %}

**损失函数的公式**

$L=k(y-m)2$

* $L$——以美元表示的单位损失（惩罚成本）

* $y$——质量变量的值

* $m$——质量变量y的目标值

* $k$——常数，取决于质量变量在财务上的重要性。

---

**损失函数的相关案例**

　　某匹萨外卖店承诺在接到订单30分钟内[送货](https://wiki.mbalib.com/wiki/%E9%80%81%E8%B4%A7)。如果超过承诺时间10分钟，那么将赔偿金额3美元。送货成本大概是2美元，但是，随着送货时间的推迟，送货成本将以0.15美元/分钟的速度下降。实际送货时间应偏离目标送货时间多长，能使成本最低？

　　总成本（$N$）=送货成本（$J$）+赔偿成本（$L$）

　　设实际送货时间为y

　　送货成本 $J=2-0.15(y-30)$

　　赔偿成本$L=k(y-30)2$

　　$N=L+J=k(y-30)2+[2-0.15(y-30)]$

　　首先求出$k$，再求出$Min(N)$。如何求$k$?

　　$L=k(y-30)2$

　　$3=k(10-0)2$

　　$\frac{3}{10^2}=0.03$

　　$Min(N)=Min \{ 0.03(y-30）2+[2-0.15(y-30)]\}$

　　求导，得出$y-30=2.5$,即实际送货时间不偏离目标时间2.5分钟以上。

{% endblockquote %}